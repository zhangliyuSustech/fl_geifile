{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import gmplot\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import h3\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def dataset(user,start,end,step):\n",
    "\n",
    "    userdata =  '../Geolife Trajectories 1.3/Data/'+user+'/Trajectory/'\n",
    "    filelist = os.listdir(userdata)  #返回指定路径下所有文件和文件夹的名字，并存放于一个列表中\n",
    "    filelist.sort()\n",
    "    names = ['lat','lng','zero','alt','days','date','time']\n",
    "    df_list = [# f为文件索引号，header为列数，names为列表列名，index_col为行索引的列编号或列名\n",
    "    pd.read_csv(userdata + f,header=6,names=names,index_col=False)\n",
    "    for f in filelist[start:end]]\n",
    "    df = pd.concat(df_list, ignore_index=True) #表格列字段不同的表合并\n",
    "    df.drop(['zero', 'days'], axis=1, inplace=True) #drop函数默认删除行，列需要加axis = 1\n",
    "    df_min = df.iloc[::step, :]\n",
    "    return df_min\n",
    "def synthetic_data(df_min):\n",
    "    a =df_min['lat'].tolist()\n",
    "    b = df_min['lng'].tolist()\n",
    "    a = torch.tensor(a,dtype=torch.float,requires_grad=True).reshape((-1, 1))\n",
    "    b = torch.tensor(b,dtype=torch.float,requires_grad=True).reshape((-1, 1))\n",
    "    features = torch.concat([a,b],1)\n",
    "    return features\n",
    "#返回（经度，纬度） shape：torch.Size([368, 2])\n",
    "\n",
    "# 这个提取出来有5个维度\n",
    "train_dataset = dataset(\"006\",0,20,20)\n",
    "test_dataset =  dataset(\"006\",20,25,20)\n",
    "# 这个提取出来有2个维度\n",
    "train_data = synthetic_data(train_dataset)\n",
    "test_data = synthetic_data(test_dataset)\n",
    "all_data =torch.concat([train_data,test_data],0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw(list,number,b,k):\n",
    "    new_list = torch.tensor(list)\n",
    "    lat = []\n",
    "    lng = []\n",
    "    for i in new_list:\n",
    "        lat.append(i[0])\n",
    "        lng.append(i[1])\n",
    "    old_lat = torch.stack(lat[:k-1])\n",
    "    old_lng =torch.stack(lng[:k-1])\n",
    "    print(old_lat)\n",
    "    print(old_lng)\n",
    "    lat_predict = (lat[k-1:])\n",
    "    lng_predict = (lng[k-1:])\n",
    "    print(lat_predict)\n",
    "    print(lng_predict)\n",
    "    lat = torch.stack(lat)\n",
    "    lng =torch.stack(lng)\n",
    "    gmap = gmplot.GoogleMapPlotter(lat[0], lng[0], b)\n",
    "    gmap.plot(lat, lng,color='b',lw=20)  #描绘轨迹点\n",
    "    gmap.plot(lat_predict, lng_predict,color='r',lw=20)  #描绘轨迹点\n",
    "    gmap.draw(\"user{}.html\".format(number))   #显示图\n",
    "    print(\"over\")\n",
    "def geo_t_h3_norepeat(data):\n",
    "        h3_list =OrderedDict()\n",
    "        for i in data:\n",
    "            a = h3.geo_to_h3(i[0], i[1], 10)\n",
    "            # print(a)\n",
    "            h3_list.setdefault(a)\n",
    "        #这这里去掉h3的重复\n",
    "        return h3_list\n",
    "def h3_t_geo(data):\n",
    "    new_list = []\n",
    "    for i in data:\n",
    "        i =h3.h3_to_geo(i)\n",
    "        new_list.append(i)\n",
    "    return new_list\n",
    "def generate_h3_list(data,label='repeat'):\n",
    "    if label=='no-repeat':\n",
    "        #不可重复的\n",
    "        alist = geo_t_h3_norepeat(data)\n",
    "        # print(type(alist))\n",
    "        LIST = list(alist.keys())\n",
    "        return np.array(LIST)\n",
    "    elif label=='repeat':\n",
    "        LIST=[]\n",
    "        for temp in data:\n",
    "            LIST.append(h3.geo_to_h3(temp[0],temp[1],10))\n",
    "        return np.array(LIST)\n",
    "    else:\n",
    "        return np.array([])\n",
    "    \n",
    "    \n",
    "    \n",
    "# 搞一个批量训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_h3_list SIZE : 1102\n",
      "Test_h3_list SIZE : 467\n",
      "14\n",
      "15\n",
      "16\n",
      "20\n",
      "195\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "202\n",
      "203\n",
      "209\n",
      "211\n",
      "245\n",
      "246\n",
      "250\n",
      "252\n",
      "260\n",
      "263\n",
      "264\n",
      "265\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "281\n",
      "282\n",
      "283\n",
      "296\n",
      "305\n",
      "318\n",
      "319\n",
      "320\n",
      "445\n",
      "459\n",
      "497\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "538\n",
      "541\n",
      "543\n",
      "544\n",
      "563\n",
      "564\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "584\n",
      "635\n",
      "640\n",
      "642\n",
      "643\n",
      "645\n",
      "649\n",
      "691\n",
      "692\n",
      "696\n",
      "698\n",
      "699\n",
      "708\n",
      "712\n",
      "715\n",
      "718\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "789\n",
      "826\n",
      "827\n",
      "828\n",
      "836\n",
      "838\n",
      "839\n",
      "845\n",
      "857\n",
      "859\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "1050\n",
      "1052\n",
      "1054\n",
      "1056\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1077\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1085\n",
      "1086\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "Train_h3_list  = generate_h3_list(train_data,label='repeat')\n",
    "Test_h3_list   = generate_h3_list(test_data,label='repeat')\n",
    "\n",
    "coincide_number= 0\n",
    "coincide = np.zeros(Train_h3_list.size)\n",
    "print(f\"Train_h3_list SIZE : {Train_h3_list.size}\")\n",
    "print(f\"Test_h3_list SIZE : {len(Test_h3_list)}\")\n",
    "\n",
    "index_for_repeat = 0\n",
    "for element in Train_h3_list :\n",
    "    index_for_repeat = index_for_repeat+1\n",
    "    if element in Test_h3_list:\n",
    "       coincide_number = coincide_number + 1\n",
    "       print(index_for_repeat)\n",
    "\n",
    "print(coincide_number)\n",
    "#这个词典\n",
    "vocab = generate_h3_list(all_data,label='no-repeat')#vocab也是h3\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False).fit(vocab.reshape(-1,1))\n",
    "\n",
    "\n",
    "#这个函数现在没有用上\n",
    "def encoding(data):\n",
    "    return encoder.transform(data.reshape(-1,1))\n",
    "def decoding(one_hot_data):\n",
    "    return encoder.inverse_transform(one_hot_data)\n",
    "\n",
    "\n",
    "# 可以通过idnex找到对应的h3\n",
    "index_h  = dict(enumerate(vocab))\n",
    "\n",
    "# 可以通过h3编码找到对应的index\n",
    "h_index ={h3:i for i ,h3 in index_h.items()}\n",
    "\n",
    "\n",
    "def label_encode(data):\n",
    "    return np.array([h_index[ch] for ch in data])\n",
    "def label_decode(data):\n",
    "    return np.array([index_h[ch] for ch in data])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#y是一个列表，k是步长，这里的数据是h3类型\n",
    "def dataloader(y,k):\n",
    "    data =[]\n",
    "    for i in range(len(y)-k+1):\n",
    "        indata = y[i:i+k]\n",
    "        outdata = y[i+k:i+k+1]\n",
    "        data.append((indata,outdata))\n",
    "    return data\n",
    "\n",
    "# def test_dataloader(y,k):\n",
    "# 现在相当于是在往前看10步\n",
    "train_dataloader = dataloader(Train_h3_list,10)\n",
    "test_dataloader =dataloader(Test_h3_list,10)\n",
    "#暂时不加批量训练\n",
    "# def data_loader(dataset,batchsize,drop_last=True):\n",
    "#     for i in range(0,len(dataset)-batchsize+1,batchsize):\n",
    "#         batch = dataset[i:i+batchsize]\n",
    "#         batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,vocab) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.num_hiddens = 320\n",
    "        self.model =nn.LSTM(\n",
    "            input_size=vocab,\n",
    "            hidden_size=self.num_hiddens,\n",
    "            batch_first=True,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.num_hiddens,vocab)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x , state):\n",
    "        r_out, states= self.model(x.view(1,10,self.vocab) ,state)\n",
    "        outdata = self.output(r_out[:,-1,:])\n",
    "        return outdata, states\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        return (torch.zeros((\n",
    "            self.model.num_layers,\n",
    "            batch_size, self.num_hiddens), device=device),\n",
    "                torch.zeros((\n",
    "                    self.model.num_layers,\n",
    "                    batch_size, self.num_hiddens), device=device))\n",
    "\n",
    "net = RNN(len(vocab)).to(device)\n",
    "# net=nn.DataParallel(net) #这个暂时先别用\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr = 0.1,momentum=0.8)\n",
    "optimizer_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "loss_function = nn.CrossEntropyLoss()#输入是没有softmax的，这个函数是自己softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]C:\\Users\\zhangliyu\\.conda\\envs\\limuconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "  0%|          | 0/200 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     37\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     38\u001B[0m a \u001B[38;5;241m=\u001B[39m encoder\u001B[38;5;241m.\u001B[39mtransform(i\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m---> 39\u001B[0m a \u001B[38;5;241m=\u001B[39m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     40\u001B[0m k \u001B[38;5;241m=\u001B[39m label_encode(k)\n\u001B[0;32m     41\u001B[0m k \u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mtensor(k)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mlong)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "top_k = 5\n",
    "\n",
    "epoches = 200\n",
    "\n",
    "def compute_acc(out , top_k):\n",
    "    probs = F.softmax(out, dim=1).squeeze()\n",
    "    probs, indices = probs.topk(top_k,largest=True) # 选取概率最大的前top_k个\n",
    "    indices = indices.cpu().numpy()\n",
    "    probs = probs.cpu().numpy()\n",
    "    char_index = np.random.choice(indices,size=1, p = probs / probs.sum()) # 随机选取一个索引\n",
    "    if not isinstance(char_index,np.ndarray):#这里没太搞懂\n",
    "        char_index = [char_index]\n",
    "    h3_value = label_decode(char_index)\n",
    "    # predict = h3_t_geo(h3_value.tolist())\n",
    "    return h3_value\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for epoch in trange(epoches):\n",
    "    loss_ =0\n",
    "    how_many_instance = 0\n",
    "    how_many_instance_right = 0\n",
    "    val_loss_ =0\n",
    "    optimizer_scheduler.step()\n",
    "    optimizer.step()\n",
    "    state =  None\n",
    "    for i , k in train_dataloader[:-1]: #这里的i和k都还是h3编码\n",
    "        k_origin = k\n",
    "        if state is None:\n",
    "            state = net.begin_state(batch_size=1, device=device)\n",
    "        else:\n",
    "            for s in state:\n",
    "                s.detach_()\n",
    "        optimizer.zero_grad()\n",
    "        a = encoder.transform(i.reshape(-1,1))\n",
    "        a =torch.tensor(a).to(torch.float32).to(device)\n",
    "        k = label_encode(k)\n",
    "        k =torch.tensor(k).to(torch.long).to(device)\n",
    "        out , state = net(a , state)\n",
    "        loss = loss_function(out,k)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ = loss_+loss\n",
    "        how_many_instance = how_many_instance + 1\n",
    "        predict = compute_acc(out.detach() , top_k)#这个char——index和k完全不是一个东西\n",
    "        # print(f\"predict : {predict} k :  {k}\")#这个一直不对\n",
    "        if predict == k_origin:#都是h3的形式\n",
    "            how_many_instance_right = how_many_instance_right + 1\n",
    "    train_loss.append(loss_.cpu().detach().numpy() / how_many_instance)\n",
    "    train_acc.append(how_many_instance_right / how_many_instance)\n",
    "    print(f'--------------Epochs{epoches} | {epoch}---------------')\n",
    "    print(f'average Train Loss : {train_loss[-1]} , train acc : {train_acc[-1]}')\n",
    "    state =  None\n",
    "    if(epoch%5==0):\n",
    "        val_ls = 0\n",
    "        how_many_instance = 0\n",
    "        how_many_instance_right = 0\n",
    "        with torch.no_grad():\n",
    "            for i , k in test_dataloader[:-1]:\n",
    "                k_origin = k\n",
    "                if state is None:\n",
    "                    state = net.begin_state(batch_size=1, device=device)\n",
    "                else:\n",
    "                    for s in state:\n",
    "                        s.detach_()\n",
    "                a = encoder.transform(i.reshape(-1,1))\n",
    "                a =torch.tensor(a).to(torch.float32).to(device)\n",
    "                k = label_encode(k)\n",
    "                k =torch.tensor(k).to(torch.long).to(device)#k的size确实是1，然后out的size是884的向量\n",
    "                out , state = net(a ,state)\n",
    "                loss = loss_function(out,k)\n",
    "                val_ls +=loss\n",
    "                predict = compute_acc(out , top_k)\n",
    "                # print(f\"char_index : {char_index} k :  {k}\")这个一直不对\n",
    "                if predict == k_origin:\n",
    "                    how_many_instance_right = how_many_instance_right + 1\n",
    "                how_many_instance = how_many_instance + 1\n",
    "                #这个取索引的方法是取出前5个然后只根据这5个概率去看\n",
    "\n",
    "\n",
    "        test_acc.append(how_many_instance_right / how_many_instance )\n",
    "        print(f'test accuracy : { test_acc[-1]} ')\n",
    "\n",
    "\n",
    "plt.plot(train_acc, label='Average Train Accuracy')\n",
    "plt.plot(train_loss, label='Average Train Loss')\n",
    "plt.plot(test_acc, label='test_acc')\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "name_id = 1\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i , k in test_dataloader[:-1]:\n",
    "\n",
    "\n",
    "        a = encoder.transform(i.reshape(-1,1))\n",
    "        a =torch.tensor(a).to(torch.float32).to(device)\n",
    "        k = label_encode(k)\n",
    "        k =torch.tensor(k).to(torch.long).to(device)\n",
    "        out = net(a)\n",
    "        loss = loss_function(out,k)\n",
    "        probs = F.softmax(out, dim=1).squeeze()\n",
    "        probs, indices = probs.topk(top_k) # 选取概率最大的前top_k个\n",
    "        indices = indices.cpu().numpy()\n",
    "        probs = probs.cpu().numpy()\n",
    "        char_index = np.random.choice(indices, p = probs / probs.sum()) # 随机选取一个索引\n",
    "        h3_value = label_decode([char_index])\n",
    "        predict = h3_t_geo(h3_value.tolist())\n",
    "        old=h3_t_geo(i.tolist())\n",
    "        total_list = old+predict\n",
    "        # print(total_list)\n",
    "        draw(total_list,name_id,20,10)\n",
    "        name_id+=1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}