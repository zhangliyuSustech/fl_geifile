{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import gmplot\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import h3\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def dataset(user,start,end,step):\n",
    "\n",
    "    userdata =  '../Geolife Trajectories 1.3/Data/'+user+'/Trajectory/'\n",
    "    filelist = os.listdir(userdata)  #返回指定路径下所有文件和文件夹的名字，并存放于一个列表中\n",
    "    filelist.sort()\n",
    "    names = ['lat','lng','zero','alt','days','date','time']\n",
    "    df_list = [# f为文件索引号，header为列数，names为列表列名，index_col为行索引的列编号或列名\n",
    "    pd.read_csv(userdata + f,header=6,names=names,index_col=False)\n",
    "    for f in filelist[start:end]]\n",
    "    df = pd.concat(df_list, ignore_index=True) #表格列字段不同的表合并\n",
    "    df.drop(['zero', 'days'], axis=1, inplace=True) #drop函数默认删除行，列需要加axis = 1\n",
    "    df_min = df.iloc[::step, :]\n",
    "    return df_min\n",
    "def synthetic_data(df_min):\n",
    "    a =df_min['lat'].tolist()\n",
    "    b = df_min['lng'].tolist()\n",
    "    a = torch.tensor(a,dtype=torch.float,requires_grad=True).reshape((-1, 1))\n",
    "    b = torch.tensor(b,dtype=torch.float,requires_grad=True).reshape((-1, 1))\n",
    "    features = torch.concat([a,b],1)\n",
    "    return features\n",
    "#返回（经度，纬度） shape：torch.Size([368, 2])\n",
    "\n",
    "# 这个提取出来有5个维度\n",
    "train_dataset = dataset(\"006\",0,20,20)\n",
    "test_dataset =  dataset(\"006\",20,25,20)\n",
    "# 这个提取出来有2个维度\n",
    "train_data = synthetic_data(train_dataset)\n",
    "test_data = synthetic_data(test_dataset)\n",
    "all_data =torch.concat([train_data,test_data],0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw(list,number,b,k):\n",
    "    new_list = torch.tensor(list)\n",
    "    lat = []\n",
    "    lng = []\n",
    "    for i in new_list:\n",
    "        lat.append(i[0])\n",
    "        lng.append(i[1])\n",
    "    old_lat = torch.stack(lat[:k-1])\n",
    "    old_lng =torch.stack(lng[:k-1])\n",
    "    print(old_lat)\n",
    "    print(old_lng)\n",
    "    lat_predict = (lat[k-1:])\n",
    "    lng_predict = (lng[k-1:])\n",
    "    print(lat_predict)\n",
    "    print(lng_predict)\n",
    "    lat = torch.stack(lat)\n",
    "    lng =torch.stack(lng)\n",
    "    gmap = gmplot.GoogleMapPlotter(lat[0], lng[0], b)\n",
    "    gmap.plot(lat, lng,color='b',lw=20)  #描绘轨迹点\n",
    "    gmap.plot(lat_predict, lng_predict,color='r',lw=20)  #描绘轨迹点\n",
    "    gmap.draw(\"user{}.html\".format(number))   #显示图\n",
    "    print(\"over\")\n",
    "def geo_t_h3_norepeat(data):\n",
    "        h3_list =OrderedDict()\n",
    "        for i in data:\n",
    "            a = h3.geo_to_h3(i[0], i[1], 10)\n",
    "            # print(a)\n",
    "            h3_list.setdefault(a)\n",
    "        #这这里去掉h3的重复\n",
    "        return h3_list\n",
    "def h3_t_geo(data):\n",
    "    new_list = []\n",
    "    for i in data:\n",
    "        i =h3.h3_to_geo(i)\n",
    "        new_list.append(i)\n",
    "    return new_list\n",
    "def generate_h3_list(data,label='repeat'):\n",
    "    if label=='no-repeat':\n",
    "        #不可重复的\n",
    "        alist = geo_t_h3_norepeat(data)\n",
    "        # print(type(alist))\n",
    "        LIST = list(alist.keys())\n",
    "        return np.array(LIST)\n",
    "    elif label=='repeat':\n",
    "        LIST=[]\n",
    "        for temp in data:\n",
    "            LIST.append(h3.geo_to_h3(temp[0],temp[1],10))\n",
    "        return np.array(LIST)\n",
    "    else:\n",
    "        return np.array([])\n",
    "    \n",
    "    \n",
    "    \n",
    "# 搞一个批量训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Train_h3_list  = generate_h3_list(train_data,label='repeat')\n",
    "Test_h3_list   = generate_h3_list(test_data,label='repeat')\n",
    "#这个词典\n",
    "vocab = generate_h3_list(all_data,label='no-repeat')#vocab也是h3\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False).fit(vocab.reshape(-1,1))\n",
    "\n",
    "\n",
    "#这个函数现在没有用上\n",
    "def encoding(data):\n",
    "    return encoder.transform(data.reshape(-1,1))\n",
    "def decoding(one_hot_data):\n",
    "    return encoder.inverse_transform(one_hot_data)\n",
    "\n",
    "\n",
    "# 可以通过idnex找到对应的h3\n",
    "index_h  = dict(enumerate(vocab))\n",
    "\n",
    "# 可以通过h3编码找到对应的index\n",
    "h_index ={h3:i for i ,h3 in index_h.items()}\n",
    "\n",
    "\n",
    "def label_encode(data):\n",
    "    return np.array([h_index[ch] for ch in data])\n",
    "def label_decode(data):\n",
    "    return np.array([index_h[ch] for ch in data])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#y是一个列表，k是步长，这里的数据是h3类型\n",
    "def dataloader(y,k):\n",
    "    data =[]\n",
    "    for i in range(len(y)-k+1):\n",
    "        indata = y[i:i+k]\n",
    "        outdata = y[i+k:i+k+1]\n",
    "        data.append((indata,outdata))\n",
    "    return data\n",
    "\n",
    "# def test_dataloader(y,k):\n",
    "# 现在相当于是在往前看10步\n",
    "train_dataloader = dataloader(Train_h3_list,10)\n",
    "test_dataloader =dataloader(Test_h3_list,10)\n",
    "#暂时不加批量训练\n",
    "# def data_loader(dataset,batchsize,drop_last=True):\n",
    "#     for i in range(0,len(dataset)-batchsize+1,batchsize):\n",
    "#         batch = dataset[i:i+batchsize]\n",
    "#         batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,vocab) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.num_hiddens = 320\n",
    "        self.model =nn.LSTM(\n",
    "            input_size=vocab,\n",
    "            hidden_size=self.num_hiddens,\n",
    "            batch_first=True,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(self.num_hiddens,vocab)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x , state):\n",
    "        r_out, states= self.model(x.view(1,10,self.vocab) ,state)\n",
    "        outdata = self.output(r_out[:,-1,:])\n",
    "        return outdata, states\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        return (torch.zeros((\n",
    "            self.model.num_layers,\n",
    "            batch_size, self.num_hiddens), device=device),\n",
    "                torch.zeros((\n",
    "                    self.model.num_layers,\n",
    "                    batch_size, self.num_hiddens), device=device))\n",
    "\n",
    "net = RNN(len(vocab)).to(device)\n",
    "# net=nn.DataParallel(net) #这个暂时先别用\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr = 0.1,momentum=0.8)\n",
    "optimizer_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "loss_function = nn.CrossEntropyLoss()#输入是没有softmax的，这个函数是自己softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]C:\\Users\\zhangliyu\\.conda\\envs\\limuconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Epochs200 | 0---------------\n",
      "average Train Loss : 6.325089339371566 , train acc : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:09<30:15,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 0.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:10<33:59, 10.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     42\u001B[0m out , state \u001B[38;5;241m=\u001B[39m net(a , state)\n\u001B[0;32m     43\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(out,k)\n\u001B[1;32m---> 44\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     46\u001B[0m loss_ \u001B[38;5;241m=\u001B[39m loss_\u001B[38;5;241m+\u001B[39mloss\n",
      "File \u001B[1;32m~\\.conda\\envs\\limuconda\\lib\\site-packages\\torch\\_tensor.py:307\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    300\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    301\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    305\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    306\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 307\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\limuconda\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    152\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m--> 154\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "top_k = 5\n",
    "\n",
    "epoches = 200\n",
    "\n",
    "def compute_acc(out , top_k):\n",
    "    probs = F.softmax(out, dim=1).squeeze()\n",
    "    probs, indices = probs.topk(top_k,largest=True) # 选取概率最大的前top_k个\n",
    "    indices = indices.cpu().numpy()\n",
    "    probs = probs.cpu().numpy()\n",
    "    char_index = np.random.choice(indices,size=1, p = probs / probs.sum()) # 随机选取一个索引\n",
    "    if not isinstance(char_index,np.ndarray):#这里没太搞懂\n",
    "        char_index = [char_index]\n",
    "    # h3_value = label_decode(char_index)\n",
    "    # predict = h3_t_geo(h3_value.tolist())\n",
    "    return char_index\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for epoch in trange(epoches):\n",
    "    loss_ =0\n",
    "    how_many_instance = 0\n",
    "    how_many_instance_right = 0\n",
    "    val_loss_ =0\n",
    "    optimizer_scheduler.step()\n",
    "    optimizer.step()\n",
    "    state =  None\n",
    "    for i , k in train_dataloader[:-1]: #这里的i和k都还是h3编码\n",
    "\n",
    "        if state is None:\n",
    "            state = net.begin_state(batch_size=1, device=device)\n",
    "        else:\n",
    "            for s in state:\n",
    "                s.detach_()\n",
    "        optimizer.zero_grad()\n",
    "        a = encoder.transform(i.reshape(-1,1))\n",
    "        a =torch.tensor(a).to(torch.float32).to(device)\n",
    "        k = label_encode(k)\n",
    "        k =torch.tensor(k).to(torch.long).to(device)\n",
    "        out , state = net(a , state)\n",
    "        loss = loss_function(out,k)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ = loss_+loss\n",
    "        how_many_instance = how_many_instance + 1\n",
    "        predict = compute_acc(out.detach() , top_k)#这个char——index和k完全不是一个东西\n",
    "        # print(f\"predict : {predict} k :  {k}\")#这个一直不对\n",
    "        if predict == k:#都是h3的形式\n",
    "            how_many_instance_right = how_many_instance_right + 1\n",
    "    train_loss.append(loss_.cpu().detach().numpy() / how_many_instance)\n",
    "    train_acc.append(how_many_instance_right / how_many_instance)\n",
    "    print(f'--------------Epochs{epoches} | {epoch}---------------')\n",
    "    print(f'average Train Loss : {train_loss[-1]} , train acc : {train_acc[-1]}')\n",
    "    state =  None\n",
    "    if(epoch%5==0):\n",
    "        val_ls = 0\n",
    "        how_many_instance = 0\n",
    "        how_many_instance_right = 0\n",
    "        with torch.no_grad():\n",
    "            for i , k in test_dataloader[:-1]:\n",
    "                if state is None:\n",
    "                    state = net.begin_state(batch_size=1, device=device)\n",
    "                else:\n",
    "                    for s in state:\n",
    "                        s.detach_()\n",
    "                a = encoder.transform(i.reshape(-1,1))\n",
    "                a =torch.tensor(a).to(torch.float32).to(device)\n",
    "                k = label_encode(k)\n",
    "                k =torch.tensor(k).to(torch.long).to(device)#k的size确实是1，然后out的size是884的向量\n",
    "                out , state = net(a ,state)\n",
    "                loss = loss_function(out,k)\n",
    "                val_ls +=loss\n",
    "                predict = compute_acc(out , top_k)\n",
    "                # print(f\"char_index : {char_index} k :  {k}\")这个一直不对\n",
    "                if predict == k:\n",
    "                    how_many_instance_right = how_many_instance_right + 1\n",
    "                how_many_instance = how_many_instance + 1\n",
    "                #这个取索引的方法是取出前5个然后只根据这5个概率去看\n",
    "\n",
    "\n",
    "        test_acc.append(how_many_instance_right / how_many_instance )\n",
    "        print(f'test accuracy : { test_acc[-1]} ')\n",
    "\n",
    "\n",
    "plt.plot(train_acc, label='Average Train Accuracy')\n",
    "plt.plot(train_loss, label='Average Train Loss')\n",
    "plt.plot(test_acc, label='test_acc')\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "name_id = 1\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i , k in test_dataloader[:-1]:\n",
    "\n",
    "\n",
    "        a = encoder.transform(i.reshape(-1,1))\n",
    "        a =torch.tensor(a).to(torch.float32).to(device)\n",
    "        k = label_encode(k)\n",
    "        k =torch.tensor(k).to(torch.long).to(device)\n",
    "        out = net(a)\n",
    "        loss = loss_function(out,k)\n",
    "        probs = F.softmax(out, dim=1).squeeze()\n",
    "        probs, indices = probs.topk(top_k) # 选取概率最大的前top_k个\n",
    "        indices = indices.cpu().numpy()\n",
    "        probs = probs.cpu().numpy()\n",
    "        char_index = np.random.choice(indices, p = probs / probs.sum()) # 随机选取一个索引\n",
    "        h3_value = label_decode([char_index])\n",
    "        predict = h3_t_geo(h3_value.tolist())\n",
    "        old=h3_t_geo(i.tolist())\n",
    "        total_list = old+predict\n",
    "        # print(total_list)\n",
    "        draw(total_list,name_id,20,10)\n",
    "        name_id+=1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}